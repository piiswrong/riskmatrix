{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:20.256471Z",
     "start_time": "2024-08-20T13:56:20.214028Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "commission = 1 / 10000.0\n",
    "USE_HOUR_DATA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROLLING_NUM = 600\n",
    "fit_window_time = pl.duration (days = ROLLING_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:20.407574Z",
     "start_time": "2024-08-20T13:56:20.260175Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "LOAD_FILE_NO_CALC = 1\n",
    "\n",
    "\n",
    "if USE_HOUR_DATA:\n",
    "    result_hour_path = \"data/agg_data_hour_to_day_alpha101.parquet\"\n",
    "    result_hour_path = \"data/agg_data_hour_to_day_all_factor.parquet\"\n",
    "else:\n",
    "    result_hour_path = 'data/result_hour.parquet'\n",
    "    result_hour_path = 'data/result_hour_alpha101.parquet'\n",
    "    # result_hour_path = 'data/daily_data_agg_from_hour_sep29.parquet'\n",
    "    result_hour_path = \"data/day_alpha101_sep29.parquet\"\n",
    "    result_hour_path = 'data/all_data_1d_boris_converted.parquet'\n",
    "    result_hour_path = 'data/all_data_1d_boris_converted_with_amihud_alpha101.parquet'\n",
    "    # result_hour_path = 'data/rolling_factors.parquet'\n",
    "    # result_hour_path = 'data/selected_factors_0916_0913.parquet'\n",
    "\n",
    "INPUT_FILE_NAME = result_hour_path.split ('/')[-1].split('.')[0]\n",
    "print (f'INPUT_FILE_NAME: {INPUT_FILE_NAME}')\n",
    "\n",
    "if LOAD_FILE_NO_CALC:\n",
    "    if os.path.exists(result_hour_path):\n",
    "        base_result_hour = pl.read_parquet(result_hour_path)\n",
    "    else:\n",
    "        assert 0, 'miss file'\n",
    "base_result_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_result_hour = pl.read_parquet('data/result_hour.parquet')\n",
    "# previous_result_hour = previous_result_hour.select(['open_time', 'symbol', 'open', 'return_skew',  'amihud', 'return_auto_corr_1_spearman_lag2'])\n",
    "# # Join previous_result_hour and result_hour on symbol and open_time columns\n",
    "# # Verify the new columns are added\n",
    "# print(selected_result_hour.columns)\n",
    "\n",
    "# previous_result_hour\n",
    "\n",
    "supplement_factor_file = []\n",
    "# supplement_factor_file.append (('data/result_hour.parquet', ['open', 'return_skew', 'amihud', 'return_auto_corr_1_spearman_lag2']))\n",
    "# supplement_factor_file.append (('data/selected_factors_0916_0913.parquet', ['rolling_count_rank_20', 'rolling_quote_volume_market_share_pct_ewm_std_6']))\n",
    "\n",
    "result_hour = base_result_hour\n",
    "\n",
    "for each_file, factor_list in supplement_factor_file:\n",
    "    supplement_result_hour = pl.read_parquet(each_file)\n",
    "    supplement_result_hour = supplement_result_hour.select(factor_list + ['open_time', 'symbol'])\n",
    "    result_hour = result_hour.join(\n",
    "        supplement_result_hour,\n",
    "        on=['open_time', 'symbol'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# # 将 open 放在第一列\n",
    "# new_col_list = result_hour.columns\n",
    "# new_col_list = [x for x in new_col_list if x != 'open']\n",
    "# new_col_list = ['open'] + new_col_list\n",
    "# result_hour = result_hour.select (new_col_list)\n",
    "\n",
    "\n",
    "result_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:20.418903Z",
     "start_time": "2024-08-20T13:56:20.403275Z"
    }
   },
   "outputs": [],
   "source": [
    "FACTOR_COMBINATION_LIST = [\"return_skew\", \"amihud\", \"return_auto_corr_1_spearman_lag2\"]\n",
    "# FACTOR_COMBINATION_LIST = ['alpha54', 'alpha55']\n",
    "for i in [13, 15, 16, 30, 33, 34, 35, 36, 45, 50, 51, 54, 55, 64, 71, 74, 99]:\n",
    "    # for i in [44, 36, 30]:\n",
    "    FACTOR_COMBINATION_LIST.append(f\"alpha{i}\")\n",
    "\n",
    "FACTOR_COMBINATION_LIST = [\"amihud\"]\n",
    "FACTOR_COMBINATION_LIST.append(\"rolling_count_rank_20\")\n",
    "FACTOR_COMBINATION_LIST.append(\"rolling_quote_volume_market_share_pct_ewm_std_6\")\n",
    "\n",
    "FACTOR_COMBINATION_LIST = [\n",
    "    \"rolling_quote_volume_market_share_pct_var_6\",\n",
    "    \"rolling_alpha24_rank_40\",\n",
    "    \"rolling_taker_buy_ratio_sum_6\",\n",
    "    \"rolling_amihud_sum_40\",\n",
    "    \"rolling_alpha24_self_cov_lag1_6\",\n",
    "    \"rolling_alpha64_self_corr_lag1_6\",\n",
    "    \"rolling_alpha40_skew_20\",\n",
    "    \"rolling_alpha40_quantile_50_6\",\n",
    "    \"rolling_alpha50_mean_20\",\n",
    "    \"rolling_return_skew_6\",\n",
    "    \"rolling_alpha64_self_corr_lag2_20\",\n",
    "    \"rolling_taker_buy_ratio_zscore_40\",\n",
    "    \"rolling_alpha46_self_cov_lag2_40\",\n",
    "    \"rolling_alpha51_self_cov_lag2_6\",\n",
    "    \"rolling_alpha30_zscore_6\",\n",
    "    \"rolling_alpha55_self_cov_lag2_6\",\n",
    "    \"rolling_alpha71_var_6\",\n",
    "    \"rolling_alpha51_quantile_50_40\",\n",
    "    \"rolling_alpha15_self_corr_lag2_20\",\n",
    "    \"rolling_alpha54_ewm_var_40\",\n",
    "    \"rolling_high_low_ratio_self_corr_lag2_20\",\n",
    "    \"rolling_alpha54_quantile_50_40\",\n",
    "    \"rolling_alpha45_var_40\",\n",
    "    \"rolling_alpha24_mean_40\",\n",
    "    \"rolling_alpha36_sum_6\",\n",
    "    \"rolling_alpha54_rank_6\",\n",
    "    \"rolling_alpha81_rank_6\",\n",
    "    \"rolling_liquidity_ratio_self_corr_lag1_6\",\n",
    "    \"rolling_alpha64_skew_6\",\n",
    "    \"rolling_count_self_cov_lag2_40\",\n",
    "]\n",
    "\n",
    "\n",
    "FACTOR_COMBINATION_LIST = ['amihud']\n",
    "for i in [13, 15, 16, 30, 33, 34, 35, 36, 45, 50, 51, 54, 55, 64, 71, 74, 99]:\n",
    "    FACTOR_COMBINATION_LIST.append(f\"alpha{i}\")\n",
    "\n",
    "FACTOR_COMBINATION_LIST = [\"return_skew\", \"amihud\", \"return_auto_corr_1_spearman_lag2\"]\n",
    "FACTOR_COMBINATION_LIST = [ \"amihud\", 'alpha30', 'alpha36', 'alpha40'] # this good\n",
    "FACTOR_COMBINATION_LIST = [  'alpha30', 'alpha36', 'alpha45', 'alpha50']\n",
    "FACTOR_COMBINATION_LIST = [ \"amihud\", 'alpha30', 'alpha36', 'alpha40'] # this good\n",
    "FACTOR_COMBINATION_LIST = [ \"amihud\", 'alpha30', 'alpha36', 'alpha40', 'alpha45', 'alpha50']\n",
    "# FACTOR_COMBINATION_LIST = ['amihud']\n",
    "print(f\"Factor Combination List: {FACTOR_COMBINATION_LIST}\")\n",
    "\n",
    "date_threshold = pl.datetime(2023, 1, 4)\n",
    "origin_xgb_x_eval = result_hour.filter(pl.col(\"open_time\") >= date_threshold).select(\n",
    "    [\"open_time\", \"symbol\", \"open\", \"close\"] + FACTOR_COMBINATION_LIST\n",
    ")\n",
    "origin_xgb_x_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:20.761303Z",
     "start_time": "2024-08-20T13:56:20.409509Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# add future_ret in result_hour\n",
    "UPDATE_POSITION_TIME = 7 # in day\n",
    "\n",
    "if USE_HOUR_DATA:\n",
    "    for i in range(1, UPDATE_POSITION_TIME + 1):\n",
    "        result_hour = result_hour.with_columns(\n",
    "            ((pl.col(\"open\").shift(-i * 24) / pl.col(\"open\") - 1) * 100)\n",
    "            .over(\"symbol\")  # Applying the function over each symbol group\n",
    "            .alias(f\"future_{i}day_return\")\n",
    "        )\n",
    "        # ).fill_null (0)\n",
    "else:\n",
    "    for i in range(1, UPDATE_POSITION_TIME + 1):\n",
    "        result_hour = result_hour.with_columns(\n",
    "            # ((pl.col(\"close\").shift(-i) / pl.col(\"close\") - 1) * 100)\n",
    "            ((pl.col(\"open\").shift(-i) / pl.col(\"open\") - 1) * 100)\n",
    "            .over(\"symbol\")  # Applying the function over each symbol group\n",
    "            .alias(f\"future_{i}day_return\")\n",
    "        )\n",
    "        # ).fill_null (0)   \n",
    "\n",
    "result_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 因子组合\n",
    "### Fama-Macbeth 线性组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:20.763298Z",
     "start_time": "2024-08-20T13:56:20.516579Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "def count_null_inf_polars(df: pl.DataFrame):\n",
    "    # Count null values\n",
    "    null_counts = df.null_count().sum()\n",
    "    \n",
    "    # Count infinity values\n",
    "    # inf_counts = df.select(\n",
    "    #     pl.sum(pl.col('*').is_infinite().cast(pl.Int64))\n",
    "    # ).item()\n",
    "    inf_counts = 0\n",
    "    \n",
    "    return null_counts, inf_counts\n",
    "\n",
    "def fama_macbeth_get_factor_weight (x_train, y_train, update_pos_days, symbol_num, factor_num):\n",
    "    factor_weight_sum = np.zeros((symbol_num, factor_num))\n",
    "\n",
    "    null_count, inf_count = count_null_inf_polars(x_train)\n",
    "    # print(f\"x_train contains {null_count} null values and {inf_count} infinity values\")\n",
    "\n",
    "    # Drop rows containing any null values\n",
    "    x_train = x_train.drop_nulls()\n",
    "\n",
    "\n",
    "    # result_hour = result_hour.sort([\"symbol\", \"open_time\"])\n",
    "    total_weights_sum = np.zeros(factor_num)\n",
    "    # unique_times = result_hour.select\n",
    "\n",
    "    unique_times = x_train.select (pl.col(\"open_time\").sort()).unique().to_numpy()\n",
    "\n",
    "    constant_sum = 0.0\n",
    "\n",
    "    for each_time in unique_times:\n",
    "        slice_data = x_train.filter(pl.col(\"open_time\") == each_time).fill_nan(0)\n",
    "        X = slice_data[FACTOR_COMBINATION_LIST].to_numpy()\n",
    "        X = sm.add_constant(X)  # 添加常数项（截距项）\n",
    "        # y = slice_data[f\"future_{UPDATE_POSITION_TIME}day_return\"].to_numpy()\n",
    "        # y = y_train[f\"future_{update_pos_days}day_return\"].to_numpy()\n",
    "        y = slice_data[f\"future_{update_pos_days}day_return\"].to_numpy()\n",
    "        # print (f' fit size: {X.shape} == {y.shape}')\n",
    "        \n",
    "        # print (each_time, X, y, X.shape, y.shape)\n",
    "\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "        weights = results.params[1:]\n",
    "        constant_sum += results.params[0] # constant term\n",
    "        # print (total_weights_sum.shape, weights.shape)\n",
    "\n",
    "        while weights.shape[0] < total_weights_sum.shape[0]: # TODO, debug here, should not use this\n",
    "            weights = np.append(weights, 0)\n",
    "            # print (each_time)\n",
    "\n",
    "        total_weights_sum += weights\n",
    "    # print ('before divide ', total_weights_sum, len (unique_times))\n",
    "    total_weights_sum /= len (unique_times)\n",
    "    avg_const_term = constant_sum / len (unique_times)\n",
    "    total_weights_sum\n",
    "\n",
    "    # for i, weight in enumerate(total_weights_sum):\n",
    "    #     print (f'{i} === {FACTOR_COMBINATION_LIST[i]} ==== {weight:.4f}')\n",
    "\n",
    "    # 计算因子相关性矩阵\n",
    "    factor_data = result_hour[FACTOR_COMBINATION_LIST].fill_nan(0)\n",
    "    # print (factor_data)\n",
    "    correlation_matrix = factor_data.corr()\n",
    "    # print (correlation_matrix)\n",
    "\n",
    "    # 绘制相关性矩阵的热图\n",
    "    if 0:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "        plt.title(f'Correlation Matrix between Factors for {update_pos_days}day')\n",
    "        plt.show()\n",
    "\n",
    "    # print (f'total weights sum {total_weights_sum}')\n",
    "    weighted_factors = [x_train[col] * weight for col, weight in zip(FACTOR_COMBINATION_LIST, total_weights_sum)]\n",
    "    # print (f'weight factors: {weighted_factors}')\n",
    "    # return weighted_factors\n",
    "    return total_weights_sum, avg_const_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:56:23.911436Z",
     "start_time": "2024-08-20T13:56:20.525178Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "factor_num = len(FACTOR_COMBINATION_LIST)\n",
    "# symbol_num = len(ret.columns) - 1\n",
    "symbol_num = result_hour[\"symbol\"].n_unique()\n",
    "\n",
    "final_result_hour = result_hour.clone()\n",
    "\n",
    "for cur_update_position_time in range(6, 8):\n",
    "    # for cur_update_position_time in range(1, UPDATE_POSITION_TIME + 1):\n",
    "    # for cur_update_position_time in range(UPDATE_POSITION_TIME, UPDATE_POSITION_TIME + 1):\n",
    "    # prepare input\n",
    "    non_nan_result_hour = result_hour.filter(\n",
    "        pl.col(f\"future_{cur_update_position_time}day_return\").is_not_nan()\n",
    "    )\n",
    "    non_nan_result_hour = non_nan_result_hour.sort([\"open_time\", \"symbol\"])\n",
    "\n",
    "    non_nan_linear_x = non_nan_result_hour.select(\n",
    "        [\"open_time\", \"symbol\", f\"future_{cur_update_position_time}day_return\"]\n",
    "        + FACTOR_COMBINATION_LIST\n",
    "    )\n",
    "    non_nan_linear_y = non_nan_result_hour.select(\n",
    "        f\"future_{cur_update_position_time}day_return\"\n",
    "    )\n",
    "\n",
    "    # print ('non_nan_linear_x: ', non_nan_linear_x)\n",
    "    # print (f'min date: {non_nan_linear_x[\"open_time\"].min()} == max date: {non_nan_linear_x[\"open_time\"].max()}')\n",
    "\n",
    "    # Get all values in non_nan_linear_x['open_time'] as a list\n",
    "    all_time_list = non_nan_linear_x[\"open_time\"].to_list()\n",
    "    cur_fit_start_date = date_threshold\n",
    "    # fit_window_time = datetime.timedelta(days=14)\n",
    "    max_date_range = non_nan_linear_x[\"open_time\"].max()\n",
    "\n",
    "    # print ('start : ', cur_fit_start_date, type (cur_fit_start_date))\n",
    "    # print (f'max date range: {max_date_range} {type (max_date_range)}')\n",
    "\n",
    "    cur_compound_factor_df = pl.DataFrame()\n",
    "\n",
    "    # start rolling fit\n",
    "    while 1:\n",
    "        # while cur_fit_start_date.to_python() < max_date_range:\n",
    "        # cur_fit_end_date = min (cur_fit_start_date + fit_window_time, max_date_range)\n",
    "        cur_fit_end_date = cur_fit_start_date + fit_window_time\n",
    "        # print (f'fit ======= {cur_fit_start_date} == {cur_fit_end_date}')\n",
    "\n",
    "        linear_x_train = non_nan_linear_x.filter(\n",
    "            pl.col(\"open_time\") < cur_fit_start_date\n",
    "        )\n",
    "        # linear_x_eval = non_nan_linear_x.filter(pl.col('open_time') >= cur_fit_start_date)\n",
    "        # cur_x_eval = origin_xgb_x_eval.filter ((pl.col ('open_time') >= cur_fit_start_date) & (pl.col ('open_time') < cur_fit_start_date + fit_window_time))\n",
    "        cur_x_eval = origin_xgb_x_eval.filter(\n",
    "            (pl.col(\"open_time\") >= cur_fit_start_date)\n",
    "            & (pl.col(\"open_time\") < cur_fit_end_date)\n",
    "        )\n",
    "\n",
    "        train_size = linear_x_train.height\n",
    "        # eval_size = linear_x_eval.height\n",
    "        eval_size = cur_x_eval.height\n",
    "        ratio = (\n",
    "            train_size / eval_size if eval_size > 0 else float(\"inf\")\n",
    "        )  # Avoid division by zero\n",
    "\n",
    "        linear_y_train = non_nan_linear_y.head(train_size)\n",
    "        linear_y_eval = non_nan_linear_y.tail(non_nan_linear_y.height - train_size)\n",
    "\n",
    "        # linear_x_train = linear_x_train.drop (['open_time', 'symbol'])\n",
    "\n",
    "        # print (linear_x_train)\n",
    "        # print ('linear y eval', linear_y_eval)\n",
    "        # print(\"Training set size:\", train_size)\n",
    "        # print(\"Evaluation set size:\", eval_size)\n",
    "        # print(\"Ratio (Train:Eval):\", ratio)\n",
    "\n",
    "        weighted_factors, const_term = fama_macbeth_get_factor_weight(\n",
    "            linear_x_train,\n",
    "            linear_y_train,\n",
    "            cur_update_position_time,\n",
    "            symbol_num=symbol_num,\n",
    "            factor_num=factor_num,\n",
    "        )\n",
    "\n",
    "        # print ('returned weight factor', weighted_factors, weighted_factors.shape, type (weighted_factors))\n",
    "\n",
    "        weighted_sum_expr = pl.lit(const_term)  # Start with the const term, then sum all factors\n",
    "        for factor, weight in zip(FACTOR_COMBINATION_LIST, weighted_factors):\n",
    "            weighted_sum_expr += pl.col(factor) * weight\n",
    "\n",
    "        # Create the 'compound_factor' column by applying the expression to the DataFrame\n",
    "        # origin_xgb_x_eval = origin_xgb_x_eval.with_columns(weighted_sum_expr.alias(f'linear_compound_factor_{cur_update_position_time}day'))\n",
    "\n",
    "        # print (f'cur x eval shape: {cur_x_eval.shape}')\n",
    "        if cur_x_eval.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        cur_col_name = f\"linear_compound_factor_{cur_update_position_time}day\"\n",
    "        cur_x_eval = cur_x_eval.with_columns(weighted_sum_expr.alias(cur_col_name))\n",
    "\n",
    "        final_result_hour = final_result_hour.with_columns(\n",
    "            pl.when(pl.col(f\"future_{cur_update_position_time}day_return\").is_null())\n",
    "            .then(pl.lit(None))\n",
    "            .otherwise(weighted_sum_expr)\n",
    "            .alias(cur_col_name)\n",
    "        )\n",
    "\n",
    "        cur_x_eval = cur_x_eval.sort(\"open_time\")\n",
    "        cur_x_eval = cur_x_eval.select(pl.col([\"open_time\", \"symbol\", cur_col_name]))\n",
    "\n",
    "        if cur_compound_factor_df.is_empty():\n",
    "            cur_compound_factor_df = cur_x_eval\n",
    "        else:\n",
    "            assert cur_compound_factor_df.columns == cur_x_eval.columns\n",
    "            cur_compound_factor_df = pl.concat([cur_compound_factor_df, cur_x_eval])\n",
    "\n",
    "        # print (f'cur x eval {cur_fit_start_date} == {cur_fit_end_date}  === . {cur_x_eval} ===== after concat: {cur_compound_factor_df}')\n",
    "        cur_fit_start_date = cur_fit_end_date\n",
    "\n",
    "    # Left join cur_compound_factor_df on origin_xgb_x_eval based on open_time\n",
    "    origin_xgb_x_eval = origin_xgb_x_eval.join(\n",
    "        cur_compound_factor_df, on=[\"open_time\", \"symbol\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 将 compound_factor 添加为新的一列\n",
    "    # origin_xgb_x_eval = origin_xgb_x_eval.with_column(compound_factor.alias(f'linear_compound_factor_{cur_update_position_time}day'))\n",
    "\n",
    "    # print(origin_xgb_x_eval)\n",
    "\n",
    "    # weighted_factors = [result_hour[col] * weight for col, weight in zip(FACTOR_COMBINATION_LIST, total_weights_sum)]\n",
    "\n",
    "    # To sum these expressions, you can't use pl.sum directly on the list. Instead, sum them using a manual reduce:\n",
    "    # from functools import reduce\n",
    "    # predict_factor_expr = reduce(lambda a, b: a + b, weighted_factors)\n",
    "\n",
    "    # # Assign the new column to the DataFrame\n",
    "    # result_hour = result_hour.with_columns(predict_factor_expr.alias(f'linear_compound_factor_{cur_update_position_time}day'))\n",
    "    # print (cur_update_position_time, result_hour)\n",
    "    # break\n",
    "# result_hour\n",
    "# origin_xgb_x_eval\n",
    "final_result_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T13:57:01.669281Z",
     "start_time": "2024-08-20T13:57:01.599046Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if USE_HOUR_DATA:\n",
    "    # origin_xgb_x_eval.write_parquet('data/linear_compound_factor_hour_data.parquet')\n",
    "    path = 'data/linear_compound_factor_hour_data.parquet'\n",
    "    zero_clock_origin_xgb_x_eval = origin_xgb_x_eval.filter(pl.col(\"open_time\").dt.hour() == 0)\n",
    "    zero_clock_origin_xgb_x_eval.write_parquet('data/linear_compound_factor_hour_zero_clock.parquet')\n",
    "else:\n",
    "    path = f'data/linear_compound_factor_rolling_{ROLLING_NUM}_input_{INPUT_FILE_NAME}.parquet'\n",
    "final_result_hour.write_parquet(path)\n",
    "print (f'write into {path}')\n",
    "\n",
    "final_result_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_hour.filter (pl.col ('symbol') == 'NOTUSDT').sort ('open_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_symbol = 'DEFIUSDT'\n",
    "# final_result_hour = origin_xgb_x_eval.clone()\n",
    "final_result_hour.filter(pl.col('symbol') == cur_symbol).sort('open_time').filter (pl.col ('linear_compound_factor_7day').is_not_null())\n",
    "final_result_hour.filter(pl.col('symbol') == cur_symbol).sort('open_time').filter (pl.col ('future_3day_return').is_not_null()).select (\n",
    "    pl.col (['open_time', 'open', 'future_3day_return', 'future_7day_return','linear_compound_factor_7day'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_hour.filter (pl.col ('symbol') == 'HNTUSDT').sort ('open_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
