{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "commission = 1 / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LOAD_FILE_NO_CALC = 1\n",
    "result_hour_path = 'data/result_hour.parquet'\n",
    "result_hour_path = 'data/rolling_factors.parquet'\n",
    "result_hour_path = \"data/agg_data_hour_to_day_alpha101.parquet\"\n",
    "\n",
    "if LOAD_FILE_NO_CALC:\n",
    "    if os.path.exists(result_hour_path):\n",
    "        result_hour = pl.read_parquet(result_hour_path)\n",
    "    else:\n",
    "        assert 0, 'miss file'\n",
    "result_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FACTOR_COMBINATION_LIST = ['amihud']\n",
    "for i in [13, 15, 16, 30, 33, 34, 35, 36, 45, 50, 51, 54, 55, 64, 71, 74, 99]:\n",
    "    FACTOR_COMBINATION_LIST.append(f\"alpha{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_threshold = pl.datetime(2023, 1, 4)\n",
    "origin_xgb_x_eval = result_hour.filter(pl.col('open_time') >= date_threshold).select(['open_time', 'symbol', 'close'] + FACTOR_COMBINATION_LIST)\n",
    "origin_xgb_x_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR_COMBINATION_LIST = [ 'return_skew',  'amihud', 'return_auto_corr_1_pearson_lag1', 'return_auto_corr_1_pearson_lag10', 'return_auto_corr_1_spearman_lag2']\n",
    "\n",
    "# add future_ret in result_hour\n",
    "UPDATE_POSITION_TIME = 7\n",
    "for i in range(1, UPDATE_POSITION_TIME + 1):\n",
    "    result_hour = result_hour.with_columns(\n",
    "        ((pl.col(\"close\").shift(-i) / pl.col(\"close\") - 1) * 100)\n",
    "        .over(\"symbol\")  # Applying the function over each symbol group\n",
    "        .alias(f\"future_{i}day_return\")\n",
    "    ).fill_null (0)\n",
    "result_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def objective(trial, xgb_x_train, xgb_y_train, xgb_x_eval, xgb_y_eval):\n",
    "    # Suggest values for the hyperparameters\n",
    "    param = {\n",
    "        'tree_method': 'gpu_hist',  # Use GPU acceleration\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.5, 0.7, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': 20000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 5, 7, 9]),\n",
    "        'random_state': 42,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'verbosity': 1,\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(xgb_x_train, label=xgb_y_train)\n",
    "    dval = xgb.DMatrix(xgb_x_eval, label=xgb_y_eval)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(param, dtrain, evals=[(dval, 'eval')], early_stopping_rounds=2000)\n",
    "    \n",
    "    # Compute RMSE on validation set\n",
    "    preds = model.predict(dval)\n",
    "    rmse = np.sqrt(mean_squared_error(xgb_y_eval, preds))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Fetch the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params['tree_method'] = 'gpu_hist'  # Ensure GPU usage\n",
    "\n",
    "dtrain = xgb.DMatrix(xgb_x_train, label=xgb_y_train)\n",
    "dval = xgb.DMatrix(xgb_x_eval, label=xgb_y_eval)\n",
    "final_model = xgb.train(best_params, dtrain, evals=[(dval, 'eval')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.xgb import xgb_model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "for cur_update_position_time in range(1, UPDATE_POSITION_TIME + 1):\n",
    "\n",
    "    non_nan_result_hour = result_hour.filter(pl.col(f\"future_{cur_update_position_time}day_return\").is_not_nan())\n",
    "\n",
    "    non_nan_xgb_x = non_nan_result_hour.select(['open_time', 'symbol'] + FACTOR_COMBINATION_LIST)\n",
    "    non_nan_xgb_y = non_nan_result_hour.select (f\"future_{cur_update_position_time}day_return\")\n",
    "\n",
    "\n",
    "    print (f'min date: {non_nan_xgb_x[\"open_time\"].min()} == max date: {non_nan_xgb_x[\"open_time\"].max()}')\n",
    "\n",
    "\n",
    "    xgb_x_train = non_nan_xgb_x.filter(pl.col('open_time') < date_threshold)\n",
    "    xgb_x_eval = non_nan_xgb_x.filter(pl.col('open_time') >= date_threshold)\n",
    "\n",
    "    train_size = xgb_x_train.height\n",
    "    eval_size = xgb_x_eval.height\n",
    "    ratio = train_size / eval_size if eval_size > 0 else float('inf')  # Avoid division by zero\n",
    "\n",
    "    xgb_y_train = non_nan_xgb_y.head(train_size)\n",
    "    xgb_y_eval = non_nan_xgb_y.tail(non_nan_xgb_y.height - train_size)\n",
    "\n",
    "\n",
    "    xgb_x_train = xgb_x_train.drop (['open_time', 'symbol'])\n",
    "    xgb_x_eval = xgb_x_eval.drop (['open_time', 'symbol'])\n",
    "\n",
    "    print(\"Training set size:\", train_size)\n",
    "    print(\"Evaluation set size:\", eval_size)\n",
    "    print(\"Ratio (Train:Eval):\", ratio)\n",
    "\n",
    "\n",
    "\n",
    "    log_period = 100 # check this later\n",
    "\n",
    "    fit_parameters = {\n",
    "        \"num_boost_round\": 20000,\n",
    "        \"early_stopping_rounds\": 2000,\n",
    "        \"verbose_eval\":  log_period\n",
    "    }\n",
    "    xgb_parameters = {\n",
    "        # \"tree_method\": \"hist\",\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"device\": \"cuda:0\",\n",
    "        \"nthread\": os.cpu_count(),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "    #   \"objective\": \"reg:pseudohubererror\",\n",
    "        \"max_depth\": 7,\n",
    "    #   \"max_depth\": 4,\n",
    "        \"subsample\": 0.8,\n",
    "    #   \"subsample\": 0.5,\n",
    "        \"colsample_bytree\": 0.1,\n",
    "    #   \"colsample_bytree\": 0.5,\n",
    "    #   \"min_child_weight\": 0.5,\n",
    "    #   \"min_child_weight\": 200,\n",
    "        \"reg_alpha\": 0.98,\n",
    "        \"reg_lambda\": 0.98,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"seed\": int(time.time()),\n",
    "        \"num_parallel_tree\": 7,\n",
    "        \"learning_rate\": 1E-2,\n",
    "        \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    model = xgb_model(\n",
    "        x_train=xgb_x_train, y_train=xgb_y_train,\n",
    "        x_val=xgb_x_eval, y_val=xgb_y_eval,\n",
    "        xgb_para=xgb_parameters, **fit_parameters\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    model.train()\n",
    "\n",
    "    # save the xgb model predict result into file\n",
    "    x_eval_factors = origin_xgb_x_eval[FACTOR_COMBINATION_LIST]\n",
    "    d_eval = xgb.DMatrix(x_eval_factors)\n",
    "    predictions = final_model.predict(d_eval)\n",
    "\n",
    "    # Add predictions back to the DataFrame to analyze or use in trading logic\n",
    "    # origin_xgb_x_eval['predicted_factor'] = predictions\n",
    "    # origin_xgb_x_eval = origin_xgb_x_eval.with_columns('predict_factor', predictions)\n",
    "    predictions_series = pl.Series(predictions)\n",
    "\n",
    "    # Add the predictions as a new column to the DataFrame\n",
    "    origin_xgb_x_eval = origin_xgb_x_eval.with_columns(predictions_series.alias(f'xgb_compound_factor_{cur_update_position_time}day'))\n",
    "\n",
    "    print (cur_update_position_time, origin_xgb_x_eval)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_xgb_x_eval.write_parquet('data/xgb_predictions.parquet')\n",
    "origin_xgb_x_eval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
